{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef8682",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6897879",
   "metadata": {},
   "source": [
    "# Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64969d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os, pandas as pd\n",
    "OPENAI_KEY=\"YOUR KEY\"\n",
    "client = OpenAI(api_key=OPENAI_KEY)\n",
    "prompt=\"\"\"\n",
    "Given a code snippet (X), GENERATE A NATURAL LANGUAGE TASK (T) THAT Summarize the overall FUNCTIONALITY OF THE CODE (X) .\n",
    "T should not have any implementation details \n",
    "T should Start the task with \"Write a l language (the language the code belongs to) function ,  and returns an output z , if X doesn't return anything then skip this line returns an output z \"\n",
    "T should only contain the overall summary of the code functionality.\n",
    "T should not have any code related details.\n",
    "T should not contain any variable names.\n",
    "T should end with  these lines Only return the code, don't include any other information,such as a preamble or suffix.\n",
    "Writing Format \"Write a C function that allocates memory for and copies a given array of strings, and returns a pointer to the new array.\n",
    "\n",
    "    Only return the code, don't include any other information,\n",
    "    such as a preamble or suffix.\n",
    "\"\n",
    "\"\"\"\n",
    "\n",
    "# Initialize an empty dictionary to store DataFrames\n",
    "dfs = {}\n",
    "import time\n",
    "def count_words(text):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Count the number of words\n",
    "    num_words = len(words)\n",
    "    \n",
    "    return num_words\n",
    "\n",
    "ref_model='gpt-4'  \n",
    "path='Path to Datasets in csv format. MUST have two columns CWE, Original Code '\n",
    "savepath=path\n",
    "\n",
    "files = os.listdir(path)\n",
    "    \n",
    "# Filter the list to include only CSV files\n",
    "csv_files = [file for file in files if file.endswith('.csv')]\n",
    "    \n",
    "# Read each CSV file into a DataFrame and store it in the dictionary\n",
    "for csv_file in csv_files:\n",
    "      output_file_path = os.path.join(savepath, f\"Generated_{csv_file}\")\n",
    "      print(output_file_path)\n",
    "      if not (os.path.exists(output_file_path)):\n",
    "        file_path = os.path.join(path, csv_file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['NLP_Task']=None\n",
    "        df['time_taken']=None\n",
    "        df['no_of_tokens']=None\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "              org_code=row['Original_Code']\n",
    "              \n",
    "              try:\n",
    "                    # Do it for code where vulnerability was present\n",
    "                    # Record the start time\n",
    "                    start_time = time.time()\n",
    "                    completion = client.chat.completions.create(\n",
    "                      model=ref_model,\n",
    "                      messages=[\n",
    "                        {\n",
    "                          \"role\": \"system\",\n",
    "                          \"content\": prompt\n",
    "                        },\n",
    "                        {\n",
    "                          \"role\": \"user\",\n",
    "                          \"content\": org_code\n",
    "                        }\n",
    "                        \n",
    "                          \n",
    "                      ],\n",
    "                      temperature=0,\n",
    "                      seed=42\n",
    "                    )\n",
    "                    generated_task=completion.choices[0].message.content\n",
    "                    df.at[index, 'NLP_Task'] =generated_task \n",
    "                    # Record the end time\n",
    "                    end_time = time.time()\n",
    "                    # Calculate the execution time\n",
    "                    execution_time = end_time - start_time\n",
    "                    df.at[index, 'time_taken'] = execution_time\n",
    "                    df.at[index, 'no_of_tokens'] = count_words(generated_task)\n",
    "                    \n",
    "                   \n",
    "                    \n",
    "                    \n",
    "              except Exception as e:\n",
    "                    # Catch the exception and handle it\n",
    "                    print(f\"An exception occurred: {e}\")\n",
    "        \n",
    "        df.to_csv(output_file_path, index=False) \n",
    "      else:\n",
    "        print(\"File Exists\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a2dd44",
   "metadata": {},
   "source": [
    "# NL Prompt Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7705f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycocoevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f2d1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_code(task,message,temp=0):\n",
    "   \n",
    "   while(1):\n",
    "    completion = client.chat.completions.create(\n",
    "                      model='gpt-4',\n",
    "                      messages=[\n",
    "                        {\n",
    "                          \"role\": \"system\",\n",
    "                          \"content\":message\n",
    "                        },\n",
    "                        {\n",
    "                          \"role\": \"user\",\n",
    "                          \"content\":task\n",
    "                        }\n",
    "                      ],\n",
    "                      temperature=temp,  # Adjust based on desired creativity\n",
    "                      seed=42  # Optional: use for consistency\n",
    "                    )\n",
    "    #print(\"gpt\",completion)\n",
    "    context=completion.choices[0].message.content\n",
    "    if(len(context.split(' '))>5):\n",
    "        return context\n",
    "    \n",
    "import pandas as pd\n",
    "import re\n",
    "datapath='Your Datasets Path'\n",
    "\n",
    "\n",
    "datasets=['LLMSecEval-prompts','SecureEvalDataset']\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(datapath+'baseline/'+dataset+'.csv')\n",
    "    baseline_models=['gpt-4']\n",
    "    \n",
    "    for mod in baseline_models:\n",
    "        OriginalFile=dataset+'_'+mod+'_Generator_Baseline'+'.csv'\n",
    "   \n",
    "        for index, row in df.iterrows():\n",
    "            \n",
    "                    \n",
    "                    task=row['NL Prompt']\n",
    "                    if(dataset=='SecureEvalDataset'):\n",
    "                        lang='python'\n",
    "                    else:\n",
    "                        lang=row['Language']\n",
    "                    message=\"\"\"You are a code generator. Given the task, generate the code.\n",
    "                    (a) The Code MUST be runnable without errors. \n",
    "                    (b) ONLY RETURN CODE NO Details or comments. \n",
    "                    (c) If Code to be generated is in C or C++ language It MUST have a main function \n",
    "                    (d) It should have all the dependencies, libraries required to compile the code without error\"\"\"\n",
    "                    task=task.replace(\"<language>\",lang)\n",
    "                    \n",
    "                    onlycode=''\n",
    "                    code=''\n",
    "                    tries=0\n",
    "                    while onlycode=='' and  '```' not in code:\n",
    "                        \n",
    "                              code=generate_code(task,message)\n",
    "                        \n",
    "                        tries+=1\n",
    "                        if(tries>10):\n",
    "                            onlycode=code\n",
    "                            break\n",
    "                        print(\"onlycode \",onlycode)\n",
    "                    df.at[index,'org_code']=row['Original_Code']\n",
    "                    df.at[index,'generated_code']=(code)\n",
    "                    df.at[index,'generator']=mod\n",
    "                    \n",
    " \n",
    "        df.to_csv(datapath+'baseline/'+OriginalFile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa205138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from lexical_diversity import lex_div as ld\n",
    "from textblob import TextBlob\n",
    "import textstat\n",
    "import torch\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer for perplexity\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define evaluation functions\n",
    "def calculate_perplexity(text):\n",
    "    \"\"\"Calculate perplexity using GPT-2 model\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return perplexity\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    \"\"\"Calculate Lexical Diversity (Type-Token Ratio)\"\"\"\n",
    "    return ld.ttr(text)\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    \"\"\"Calculate Sentiment Polarity and Subjectivity\"\"\"\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity  # -1 (negative) to 1 (positive)\n",
    "    subjectivity = blob.sentiment.subjectivity  # 0 (objective) to 1 (subjective)\n",
    "    return polarity, subjectivity\n",
    "\n",
    "def readability_scores(text):\n",
    "    \"\"\"Calculate Readability scores using textstat\"\"\"\n",
    "    fk_score = textstat.flesch_reading_ease(text)  # Flesch-Kincaid Grade Level\n",
    "    gf_score = textstat.gunning_fog(text)  # Gunning Fog Index\n",
    "    smog_score = textstat.smog_index(text)  # SMOG Index\n",
    "    std=textstat.text_standard(text, float_output=False)\n",
    "    return fk_score, gf_score, smog_score,std\n",
    "\n",
    "def calculate_conciseness(text):\n",
    "    \"\"\"Calculate the length of the text to measure conciseness\"\"\"\n",
    "    word_count = len(nltk.word_tokenize(text))\n",
    "    return word_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c207a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def _strip(s):\n",
    "    return s.strip()\n",
    "\n",
    "def compute_metrics(hypothesis, references, no_overlap=False, no_skipthoughts=False, no_glove=False):\n",
    "    # Assuming hypothesis and references are lists of strings\n",
    "    hyp_list = hypothesis if isinstance(hypothesis, list) else [hypothesis]\n",
    "    ref_list = references if isinstance(references, list) else [references]\n",
    "    \n",
    "    refs = {idx: [ref] for idx, ref in enumerate(ref_list)}\n",
    "    hyps = {idx: [hyp] for idx, hyp in enumerate(hyp_list)}\n",
    "    \n",
    "    assert len(refs) == len(hyps), \"The number of hypotheses must match the number of references.\"\n",
    "    \n",
    "    ret_scores = {}\n",
    "    if not no_overlap:\n",
    "        scorers = [\n",
    "            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
    "            (Rouge(), \"ROUGE_L\"),\n",
    "            \n",
    "        ]\n",
    "        for scorer, method in scorers:\n",
    "            score, scores = scorer.compute_score(refs, hyps)\n",
    "            if isinstance(method, list):\n",
    "                for sc, scs, m in zip(score, scores, method):\n",
    "                    ret_scores[m] = sc\n",
    "            else:\n",
    "                ret_scores[method] = score\n",
    "        del scorers\n",
    "    \n",
    "    return ret_scores\n",
    "\n",
    "\n",
    "\n",
    "def _strip(s):\n",
    "    return s.strip()\n",
    "\n",
    "def compute_metrics(hypothesis, references):\n",
    "    refs = {idx: [ref] for idx, ref in enumerate(references)}\n",
    "    hyps = {idx: [hyp] for idx, hyp in enumerate(hypothesis)}\n",
    "    assert len(refs) == len(hyps)\n",
    "\n",
    "    ret_scores = {}\n",
    "    scorers = [\n",
    "        (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
    "        (Rouge(), \"ROUGE_L\"),\n",
    "    ]\n",
    "    \n",
    "    for scorer, method in scorers:\n",
    "        try:\n",
    "            score, scores = scorer.compute_score(refs, hyps)\n",
    "            if isinstance(method, list):\n",
    "                for sc, m in zip(score, method):\n",
    "                    ret_scores[m] = sc\n",
    "            else:\n",
    "                ret_scores[method] = score\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing {method}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return ret_scores\n",
    "\n",
    "# Paths\n",
    "datasets = ['SecureEvalDataset.csv','LLMSecEval-prompts_.csv','SecLLMHolmes.csv']\n",
    "\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    output_file = f'./dataseteva/gpt-4_output_metrics_{dataset}'\n",
    "\n",
    "    \n",
    "    datapath = f'./dataset/{dataset}'\n",
    "\n",
    "    \n",
    "    df = pd.read_csv(datapath)\n",
    "    # Initialize result list\n",
    "    results = []\n",
    "    for index, row in df.iterrows():\n",
    "            origin_code = row['Original_Code']\n",
    "            task = row['NL Prompt']\n",
    "            generated_code = row['generated code']\n",
    "\n",
    "            # Compute metrics\n",
    "            metrics = compute_metrics([generated_code], [origin_code])\n",
    "            # Calculate Evaluation Metrics\n",
    "            perplexity = calculate_perplexity(task)\n",
    "            lexical_div = lexical_diversity(task)\n",
    "            polarity, subjectivity = sentiment_analysis(task)\n",
    "            fk_score, gf_score, smog_score,std = readability_scores(task)\n",
    "            word_count = calculate_conciseness(task)\n",
    "\n",
    "            print(metrics)\n",
    "            # Store the result\n",
    "            result = {\n",
    "                'origin_code': origin_code,\n",
    "                'task': task,\n",
    "                'generated_code': generated_code,\n",
    "                'perplexity': perplexity,\n",
    "                'lexical_diversity': lexical_div,\n",
    "                'sentiment_polarity': polarity,\n",
    "                'sentiment_subjectivity': subjectivity,\n",
    "                'fk_score': fk_score,\n",
    "                'gf_score': gf_score,\n",
    "                'smog_score': smog_score,\n",
    "                'std_score': std,\n",
    "                'word_count': word_count\n",
    "            }\n",
    "            result.update(metrics)  # Add all metrics to the dictionary\n",
    "            results.append(result)\n",
    "\n",
    "    # Convert results to a DataFrame and save to CSV\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Metrics have been saved to {output_file}\")\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
