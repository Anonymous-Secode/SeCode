# NL Prompt Dataset for Vulnerable Code Evaluation

This repository provides a natural language (NL) prompt dataset designed to evaluate the security of code generated by large language models (LLMs) when given high-level tasks from novice developers. The dataset focuses on assessing how LLMs handle vulnerabilities in generated code, based on real-world examples from three state-of-the-art (SOTA) vulnerable code datasets.

## Dataset Contents

Each dataset in this repository includes the following components:

1. **Natural Language Prompts (NL Prompts)**  
   A translation of the original vulnerable code into a high-level natural language task description. These prompts reflect the type of requests a novice developer might give to an LLM.

2. **CWE Identifiers (Ground Truth)**  
   Each entry in the dataset is annotated with the corresponding CWE (Common Weakness Enumeration) ID, providing the ground truth for the type of vulnerability present in the original code.

3. **Original Vulnerable Code**  
   The actual code containing vulnerabilities, as found in the original datasets. This serves as the basis for both the natural language prompts and the associated CWE annotations.

This dataset is ideal for researchers and developers looking to test LLMs' ability to generate secure code in response to ambiguous or imprecise natural language task descriptions.

## Original Code Datasets

1. LLMSecEval: [https://github.com/moyix/AsleepKeyboardDataset/tree/main/data/original](https://github.com/moyix/AsleepKeyboardDataset/tree/main/data/original) (C and Python)  
2. SecureEval: [https://github.com/s2e-lab/SecurityEval](https://github.com/s2e-lab/SecurityEval) (Python)  
3. SecHolmes: [https://github.com/ai4cloudops/SecLLMHolmes/tree/main/datasets](https://github.com/ai4cloudops/SecLLMHolmes/tree/main/datasets) (C and Python)
